\documentclass[12pt]{extarticle}
%Some packages I commonly use.
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{framed}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}
\usepackage[top=1 in,bottom=1in, left=1 in, right=1 in]{geometry}

%A bunch of definitions that make my life easier
\newcommand{\matlab}{{\sc Matlab} }
\newcommand{\cvec}[1]{{\mathbf #1}}
\newcommand{\rvec}[1]{\vec{\mathbf #1}}
\newcommand{\ihat}{\hat{\textbf{\i}}}
\newcommand{\jhat}{\hat{\textbf{\j}}}
\newcommand{\khat}{\hat{\textbf{k}}}
\newcommand{\minor}{{\rm minor}}
\newcommand{\trace}{{\rm trace}}
\newcommand{\spn}{{\rm Span}}
\newcommand{\rem}{{\rm rem}}
\newcommand{\ran}{{\rm range}}
\newcommand{\range}{{\rm range}}
\newcommand{\mdiv}{{\rm div}}
\newcommand{\proj}{{\rm proj}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\attn}[1]{\textbf{#1}}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem*{definition}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{note}{Note}
\newtheorem{exercise}{Exercise}
\newcommand{\bproof}{\bigskip {\bf Proof. }}
\newcommand{\eproof}{\hfill\qedsymbol}
\newcommand{\Disp}{\displaystyle}
\newcommand{\qe}{\hfill\(\bigtriangledown\)}
\setlength{\columnseprule}{1 pt}


\title{Rienforcement learning: Assignment 2}

\author{
  Oluwatomilayo, Adegbite\\
  \texttt{500569283}
  \and
  Nikolas, Maier\\
  \texttt{500461990}
}

\begin{document}

\maketitle

\section{}
\subsection{Exercise 3.17}
Must give action value $q_\pi$(s,a)  in terms of $q_\pi$(s',a') \\ 
 \begin{equation}
V_\pi(s) =  \sum_a \pi(s,a)  \sum P_{s s'}^ a (R_{s s'}^ a +  \alpha V_\pi(s')) \\ \\
\end{equation} 
The value function represented as a bellman equation taking state and summing over all possible actions in that state and value of successor states 

 \begin{equation}
 P_{s s'}^ a = p(s'  \mid s,a) = Pr\left\{ S_t = s' \mid S_{t-1} = s, A_{t-1} = a \right\} = \sum_{r \in R} p(s', r \mid s,a) 
\end{equation} 
Equation to represent probability of state transition given action and previous state is the same as probability of moving to successor state and receiving reward from future state and action

 \begin{equation}
G_t = p(s'  \mid s,a) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}  + ...  = \sum_{k=0}^{ \infty} \gamma^k R_{t+k+1} 
\end{equation} 

$G_t$ represents the discounted future reward 

 \begin{equation}
\begin{split}
q_\pi(s,a)  =  E_\pi [ G_t  \mid S_t = s , A_t = s   ]  \\  \\
  =  E_\pi [  \sum_{k=0}^{ \infty} \gamma^k R_{t+k+1}  \mid S_t = s , A_t = s   ]  \\ \\
  =  E_\pi [  \sum_{k=0}^{ \infty} \gamma^k R_{t+k+1}  \mid S_t = s , A_t = s   ]  \\ \\
q_\pi(s,a) = E_\pi [  \sum_{s'}P_{s s'}^ a (R_{s s'}^ a +  \sum_{a'} \alpha  q_\pi(s',a'))   \mid S_t = s , A_t = s   ]
\end{split}
\end{equation} 



  To get the value of all possible successor states sum over s', multiply the probability of moving from current state to next state given action a by the result of the reward of that action and state transition and the sum of all possible actions in s' times a discount times the value of $q_\pi$(s',a')

\subsection{Exercise 3.19} 

 \begin{equation}
\begin{split}
G_t = p(s'  \mid s,a) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4}  + ...   \\ 
 =  R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3}  + \gamma^2 R_{t+4} + ...)  \\
 = R_{t+1} + \gamma G_{t+1}
\end{split}
\end{equation} 

$G_t$ can be represented as current reward + discounted future reward 

 \begin{equation}
q_\pi(s,a) =  E[G_t \mid S_t = s ,  A_t = s   ] \\   \\
\end{equation} 

 \begin{equation}
q_\pi(s,a) =   E [R_{t+1} + \alpha V_\pi( s'_{t+1})   \mid S_t = s , A_t = s   ] \\  \\
\end{equation} 

$q_\pi(s,a)$ Given in terms of future expected reward considering discount and expected value of future states

\end{document}